{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podstawy uczenia maszynowego - tutorial 2: Feature selection and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie zbiorów danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Zbiór danych medycznych pacjentów pozwalający na diagnostykę raka płuc\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "cancer.data = scale(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zbiór danych pozwalający przewidywać, czy małżeństwo się rozpadnie na podstawie odpowiedzi (w skali 0-4) na 56 pytań\n",
    "\n",
    "divorces = pd.read_csv('./datasets/divorce.csv', sep=';')\n",
    "\n",
    "# treści pytań są przechowywane w osobnym pliku\n",
    "with open('./datasets/questions.csv') as file:\n",
    "    questions = np.array(file.read().split('\\n'))\n",
    "\n",
    "divorces = {\n",
    "    'data': divorces.drop(labels=['Class'], axis=1).astype(float),\n",
    "    'target': list(divorces['Class']),\n",
    "    'feature_names': questions\n",
    "}\n",
    "\n",
    "divorces['data'] = scale(divorces['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selekcja cech to ograniczenie liczby atrybutów danych, na których pracuje model, przez odrzucenie najmniej użytecznych z nich. Taka rezygnacja z części danych pozwala osiągnąć konkretne korzyści:\n",
    "* redukcja wymiarów - odrzucenie części cech oznacza zmniejszenie wymiarowości problemu, co ułatwia uniknięcie przetrenowania modelu\n",
    "* uproszczenie modelu - model pracujący na mniejszej liczbie cech jest bardziej zrozumiały i łatwiej identyfikować w nim problemy\n",
    "* lepsze wyniki - cechy niezwiązane z badaną właściwością mogą w sposób losowy zaburzać otrzymywane wyniki\n",
    "* poprawa wydajności - mało istotne cechy są przetwarzane niepotrzebnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody Selekcji cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Odrzucenie cech niecharakterystycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cechy, które wykazują niewielką zmienność pomiędzy próbkami, prawdopodobnie nie niosą szczególnie użytecznej informacji - zazwyczaj nie można na ich podstawie dokonać klasyfikacji (choć warto zachować ostrożność przy zbiorach danych o silnej dysproporcji pomiędzy licznościami klas oraz cechach mogących przyjmować wartości z niewielkiego zakresu). Cechy takie można rozpoznać po niewielkiej wariancji - stąd najprostsze podejście do selekcji cech może polegać na ich przefiltrowaniu i odrzuceniu tych o zbyt niskiej wariancji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# variance should equal, because ...\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "cancer.data = selector.fit_transform(cancer.data)\n",
    "\n",
    "\n",
    "# lista atrybutów wymaga zaktualizowania, selector.get_support() zwraca indeksy wybranych cech\n",
    "cancer['feature_names'] = cancer['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "selector = VarianceThreshold()\n",
    "divorces['data'] = selector.fit_transform(divorces['data'])\n",
    "\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Odrzucenie cech niezwiązanych z badaną właściwością"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy spróbować przewidzieć, na ile każda z cech jest związana z badaną właściwością, i odrzucić te, dla których związek jest luźny. Możliwe są dwa zasadnicze podejścia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Obliczenie korelacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najprostszym sposobem określenia, czy cecha jest związana z inną (w szczególności - przynależnością do danej klasy) jest obliczenie korelacji między nimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_correlation(X, Y, feature_names, n_features):\n",
    "    scores = [abs(np.corrcoef(feature, Y))[0, 1] for feature in X.T]\n",
    "    selected_indices = np.argsort(scores)[:n_features]\n",
    "    data = X.T[selected_indices].T\n",
    "    selected_feature_names = feature_names[selected_indices]\n",
    "    return data, selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cancer.data, cancer.feature_names = filter_correlation(cancer.data, cancer.target, cancer.feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "divorces['data'], divorces['feature_names'] = \\\n",
    "    filter_correlation(divorces['data'], divorces['target'], divorces['feature_names'], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Inne statystyki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatywnie, możemy wykorzystać inną statystykę, której wartość rośnie wraz ze wzrostem różnicy wartości w dwóch grupach - np. Chi-kwadrat. \n",
    "\n",
    "W tym celu zakładamy, że badana cecha (nazwijmy ją A) nie ma związku z przynależnością próbki do określonej klasy (oznaczmy ją przez C). Opierając się na tym założeniu, obliczamy, ile próbek z każdą możliwą wartością cechy A powinno należeć do klasy C (jako liczba próbek o danej wartości cechy A * liczba próbek w ekperymencie należących do klasy C / liczba wszystkich próbek),\n",
    "i obliczamy wartość wybranej statystyki na podstawie różnic pomiędzy otrzymanymi wartościami a rzeczywistymi danymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=20)\n",
    "\n",
    "cancer.data = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=30)\n",
    "\n",
    "divorces['data'] = selector.fit_transform(divorces['data'], divorces['target'])\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Próbne wytrenowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Odfiltrowanie cech nieznaczących"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast \"ręcznie\" znajdować i usuwać mało istotne cechy, możemy spróbować wytrenować na naszych danych model, który ucząc się \"przy okazji\" zapisuje istotność cech, i usunąć te, które nie mają dużego znaczenia w podejmowaniu decyzji przez model. Aby zastosować takie podejści, konieczny jest wybór modelu, który udostępnia istotność cech - w przypadku Scikit-learn są to modele posiadające pole coef_ lub feature_importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "classifier = classifier.fit(cancer.data, cancer.target)\n",
    "selector = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "cancer.data = selector.transform(cancer.data)\n",
    "cancer.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "classifier = classifier.fit(divorces['data'], divorces['target'])\n",
    "selector = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "divorces['data'] = selector.transform(divorces['data'])\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Rekurencyjny wybór zadanej liczby najlepszych cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli chcemy ograniczyć liczbę cech do konkretnej wartości, powyższe podejście można zmodyfikować: zamiast wybierać cechy powyżej określonej granicy, lepszym podejściem może być odrzucenie najsłabszej z cech (lub kilku najgorszych) i rekurencyjne powtarzanie procesu, aż do osiągnięcia zadanej ich liczby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimator = ExtraTreesClassifier(n_estimators=50)\n",
    "selector = RFE(estimator, 20, step=3)\n",
    "\n",
    "cancer.data = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimator = ExtraTreesClassifier(n_estimators=50)\n",
    "selector = RFE(estimator, 30, step=3)\n",
    "\n",
    "divorces['data'] = selector.fit_transform(divorces['data'], divorces['target'])\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macierz kowariancji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wariancja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wariancja** jest intuicyjnie utożsamiana ze zróżnicowaniem zbiorowości;\n",
    "jest średnią arytmetyczną kwadratów różnic poszczególnych wartości cechy od wartości oczekiwanej.\n",
    "Wariancję zmiennej losowej X oznaczamy jako:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Var(X)\\newline\n",
    "D^2(X)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obliczamy za pomocą wzoru:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "D^2(X) = E(X^2) - [E(X)]^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdzie E[X] jest wartością oczekiwaną zmiennej losowej X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kowariancja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kowariancją** nazywamy zależnośc liniową między dwowa zmiennymi losowymi X i Y.\n",
    "Kowariancję oznaczamy jako:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "cov(X,Y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wyliczamy ze wzoru:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "cov(X,Y) = E(X * Y) - E(X) * E(Y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macierz kowariancji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sposób wyliczania\n",
    "**Macierz kowariancji** jest uogólnieniem pojęcia wariancji dla przypadków wielowymiarowych. Dla wektora losowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(X_1,X_2,...,X_n)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma ona postać:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\Sigma =  \\begin{vmatrix}\n",
    "\\ \\sigma^2_1 & \\sigma_{12} & ... & \\sigma_{1n}  \\\\\n",
    "\\ \\sigma_{21} & \\sigma^2_2 & ... & \\sigma_{2n}  \\\\\n",
    "\\ ... & ... & ... & ... \\\\\n",
    "\\ \\sigma_{n1} & \\sigma_{n2} & ... & \\sigma^2_n\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdzie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma^2_i = D^2(X_i) - wariancja\\; zmiennej\\; X_i \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma_{ij} = cov(X_i, X_j) - kowariancja\\; między\\; zmiennymi\\; losowymi\\; X_i\\; i\\; X_j\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Macierz kowariancji w Pythonie\n",
    "\n",
    "Do wyznaczania macierzy kowariancji używamy funkcji cov z biblioteki numpy. Wylicza ona macierz na podstawie podanych tablic i wag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>numpy.cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> m - jedno- lub dwu- wymiarowa tablic danych, zawiarająca różne zmienne i obserwacje</li>\n",
    "    <li> y - dodatkowy zbiór danych, musi być takiej wielkości jak m</li>\n",
    "    <li>rowvar - jeżeli ustawione na True to każdy wiersz m odpowiada za jedną zmienną, a każda kolumna za obserwacje; gdy ustawione na False jest na odwrót </li>\n",
    "    <li>bias - odpowiada za rodzaj normalizacji</li>\n",
    "    <li>ddof - jeżeli inne niż None nadpisuje odpowiednio wartości zwrócone przez bias</li>\n",
    "    <li>fweights - jednowymiarowa tablica liczb całkowitych, wyznaczająca wagi częstotliwości, czyli ile razy dana obserwacja powinna być powtórzona</li>\n",
    "    <li>aweights - jednowymiarowa tablica, odpowiedzialna za wagi (\"ważność\") danych obserwacji.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Działanie funkcji numpy.cov\n",
    "\n",
    "Mamy daną tablicę m, gdzie kolumny są poszeczególnymi obserwacjami, niech f = fweight i a = aweight. Wyliczanie macierzy kowariancji następuje w podany sposób:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> w = f * a<br>\n",
    "=> v1 = np.sum(w)<br>\n",
    "=> v2 = np.sum(w * a)<br>\n",
    "=> m -= np.sum(m * w, axis=1, keepdims=True) / v1<br>\n",
    "=> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cancer.data[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.cov(cancer.data[0:3,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cancer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heat mapa** jest graficzną reprezentacją danych, gdzie każdy wartość elementu macierzy jest reprezentowana przez dany kolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"heat_map.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "matrix_data = np.cov(divorces['data'])\n",
    "heatmap_data = matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "heatmap_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(heatmap_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Heat mapy pozwalają na łatwiejsze znalezienie obszarów o większym znaczeniu w przypadku dużej ilości danych. Są one łatwiejsze do przeanalizowania niż surowe dane liczbowe.\n",
    "\n",
    "Z takiej heat mapy możemy odczytać jak dużą kowariancją cechują się dwie zmienne losowe. Duża kowariancja między dwiema zmiennymi wskazuje, że są one wysoce „skorelowane” - zawierają informacje, które można przewidzieć lub przedstawić pojedynczą zmienną.\n",
    "\n",
    "Klasyfikowanie dużych danych bywa czasochłonne i zasobożerne. Informacje, które można wywnioskować z heat mapy pozwalają nam wyłonić z pełnego zbioru danych odpowiedni fragment, który następnie posłuży do tworzenia bardziej optymalnych klasyfikatorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Transformacja PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacja PCA jest algorytmem analizy danych. Wykorzystuje informacje o powiązaniach pomiędzy danymi wejściowymi. Umożliwia to dokonanie selekcji i \"kompresji\" danych bez utraty istotnych informacji pierwotnego zestawu danych. PCA może być wykorzystane właśnie w problemach kompresji, analizy oraz przetwarzania złożonych zbiorów danych tak, aby wyłuszczyć z nich składaniki o największej zmienności i największym wpływie na pozostałe informacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
