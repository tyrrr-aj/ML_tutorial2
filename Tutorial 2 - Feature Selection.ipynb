{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podstawy uczenia maszynowego - tutorial 2: Feature selection and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie zbiorów danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Zbiór danych medycznych pacjentów pozwalający na diagnostykę raka płuc\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "cancer.data = scale(cancer.data)\n",
    "\n",
    "cancer_reduced = deepcopy(cancer)\n",
    "\n",
    "def get_cancer_displayable(n_rows=20):\n",
    "    return pd.DataFrame([np.append(sample, cancer.target[i]) for i, sample in enumerate(cancer.data)],\n",
    "             columns=np.append(cancer.feature_names, 'had_cancer'))[:n_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_cancer_displayable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zbiór danych pozwalający przewidywać, czy małżeństwo się rozpadnie na podstawie odpowiedzi (w skali 0-4) na 56 pytań\n",
    "\n",
    "divorces = pd.read_csv('./datasets/divorce.csv', sep=';')\n",
    "\n",
    "# treści pytań są przechowywane w osobnym pliku\n",
    "with open('./datasets/questions.csv') as file:\n",
    "    questions = np.array(file.read().split('\\n'))\n",
    "\n",
    "divorces = {\n",
    "    'data': divorces.drop(labels=['Class'], axis=1).astype(float),\n",
    "    'target': list(divorces['Class']),\n",
    "    'feature_names': questions\n",
    "}\n",
    "\n",
    "divorces['data'] = scale(divorces['data'])\n",
    "\n",
    "divorces_reduced = deepcopy(divorces)\n",
    "\n",
    "def get_divorces_displayable(n_rows=20):\n",
    "    return pd.DataFrame([np.append(sample, divorces['target'][i]) for i, sample in enumerate(divorces['data'])],\n",
    "             columns=np.append(divorces['feature_names'], 'divorced'))[:n_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_divorces_displayable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selekcja cech to ograniczenie liczby atrybutów danych, na których pracuje model, przez odrzucenie najmniej użytecznych z nich. Taka rezygnacja z części danych pozwala osiągnąć konkretne korzyści:\n",
    "* redukcja wymiarów - odrzucenie części cech oznacza zmniejszenie wymiarowości problemu, co ułatwia uniknięcie przetrenowania modelu\n",
    "* uproszczenie modelu - model pracujący na mniejszej liczbie cech jest bardziej zrozumiały i łatwiej identyfikować w nim problemy\n",
    "* lepsze wyniki - cechy niezwiązane z badaną właściwością mogą w sposób losowy zaburzać otrzymywane wyniki\n",
    "* poprawa wydajności - mało istotne cechy są przetwarzane niepotrzebnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody Selekcji cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Odrzucenie cech niecharakterystycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cechy, które wykazują niewielką zmienność pomiędzy próbkami, prawdopodobnie nie niosą szczególnie użytecznej informacji - zazwyczaj nie można na ich podstawie dokonać klasyfikacji (choć warto zachować ostrożność przy zbiorach danych o silnej dysproporcji pomiędzy licznościami klas oraz cechach mogących przyjmować wartości z niewielkiego zakresu). Cechy takie można rozpoznać po niewielkiej wariancji - stąd najprostsze podejście do selekcji cech może polegać na ich przefiltrowaniu i odrzuceniu tych o zbyt niskiej wariancji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# variance should equal, because ...\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "cancer_reduced.data = selector.fit_transform(cancer.data)\n",
    "\n",
    "\n",
    "# lista atrybutów wymaga zaktualizowania, selector.get_support() zwraca indeksy wybranych cech\n",
    "cancer_reduced['feature_names'] = cancer['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "selector = VarianceThreshold()\n",
    "divorces_reduced['data'] = selector.fit_transform(divorces['data'])\n",
    "\n",
    "divorces_reduced['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Odrzucenie cech niezwiązanych z badaną właściwością"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy spróbować przewidzieć, na ile każda z cech jest związana z badaną właściwością, i odrzucić te, dla których związek jest luźny. Możliwe są dwa zasadnicze podejścia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Obliczenie korelacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najprostszym sposobem określenia, czy cecha jest związana z inną (w szczególności - przynależnością do danej klasy) jest obliczenie korelacji między nimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_correlation(X, Y, feature_names, n_features):\n",
    "    scores = [abs(np.corrcoef(feature, Y))[0, 1] for feature in X.T]\n",
    "    selected_indices = np.argsort(scores)[:n_features]\n",
    "    data = X.T[selected_indices].T\n",
    "    selected_feature_names = feature_names[selected_indices]\n",
    "    return data, selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cancer_reduced.data, cancer_reduced.feature_names = filter_correlation(cancer.data, cancer.target, cancer.feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "divorces_reduced['data'], divorces_reduced['feature_names'] = \\\n",
    "    filter_correlation(divorces['data'], divorces['target'], divorces['feature_names'], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Inne statystyki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatywnie, możemy wykorzystać inną statystykę, której wartość rośnie wraz ze wzrostem różnicy wartości w dwóch grupach - np. ANOVA (częstym wyborem jest miara Chi-Square, nie nadaje się ona jednak do znormalizowanych danych - nie dopuszcza wartości ujemnych, konieczne więc byłoby dodatkowe przesunięcie danych). \n",
    "\n",
    "W tym celu zakładamy, że badana cecha (nazwijmy ją A) nie ma związku z przynależnością próbki do określonej klasy (oznaczmy ją przez C). Opierając się na tym założeniu, obliczamy, ile próbek z każdą możliwą wartością cechy A powinno należeć do klasy C (jako liczba próbek o danej wartości cechy A * liczba próbek w ekperymencie należących do klasy C / liczba wszystkich próbek),\n",
    "i obliczamy wartość wybranej statystyki na podstawie różnic pomiędzy otrzymanymi wartościami a rzeczywistymi danymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=20)\n",
    "\n",
    "cancer_reduced.data = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer_reduced.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=30)\n",
    "\n",
    "divorces_reduced['data'] = selector.fit_transform(divorces['data'], divorces['target'])\n",
    "divorces_reduced['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Próbne wytrenowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Odfiltrowanie cech nieznaczących"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast \"ręcznie\" znajdować i usuwać mało istotne cechy, możemy spróbować wytrenować na naszych danych model, który ucząc się \"przy okazji\" zapisuje istotność cech, i usunąć te, które nie mają dużego znaczenia w podejmowaniu decyzji przez model. Aby zastosować takie podejści, konieczny jest wybór modelu, który udostępnia istotność cech - w przypadku Scikit-learn są to modele posiadające pole coef_ lub feature_importances_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "classifier = classifier.fit(cancer.data, cancer.target)\n",
    "selector = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "cancer_reduced.data = selector.transform(cancer.data)\n",
    "cancer_reduced.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "classifier = classifier.fit(divorces['data'], divorces['target'])\n",
    "selector = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "divorces_reduced['data'] = selector.transform(divorces['data'])\n",
    "divorces_reduced['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Rekurencyjny wybór zadanej liczby najlepszych cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli chcemy ograniczyć liczbę cech do konkretnej wartości, powyższe podejście można zmodyfikować: zamiast wybierać cechy powyżej określonej granicy, lepszym podejściem może być odrzucenie najsłabszej z cech (lub kilku najgorszych) i rekurencyjne powtarzanie procesu, aż do osiągnięcia zadanej ich liczby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimator = ExtraTreesClassifier(n_estimators=50)\n",
    "selector = RFE(estimator, 20, step=3)\n",
    "\n",
    "cancer_reduced.data = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer_reduced.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimator = ExtraTreesClassifier(n_estimators=50)\n",
    "selector = RFE(estimator, 30, step=3)\n",
    "\n",
    "divorces_reduced['data'] = selector.fit_transform(divorces['data'], divorces['target'])\n",
    "divorces_reduced['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wizualizację ważności poszczególnych cech najłatwiej jest oprzeć o próbne wytrenowanie modelu i odczytanie wartości z pola *feature_importances_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "def plot_feature_importances(classifier, names, size=None):\n",
    "    sorted_indices = np.argsort(classifier.feature_importances_)\n",
    "    labels = names[sorted_indices][::-1]\n",
    "    Y = classifier.feature_importances_[sorted_indices][::-1]\n",
    "    X = [i for i in range(len(labels))]\n",
    "    if size is not None:\n",
    "        plt.figure(figsize = size)\n",
    "    plt.bar(X, Y, tick_label=labels)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.fit(cancer.data, cancer.target)\n",
    "plot_feature_importances(classifier, cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.fit(divorces['data'], divorces['target'])\n",
    "\n",
    "# treści pytań są długie, wykres etykietujemy więc samymi ich numerami\n",
    "labels = np.array(['question' + str(i) for i in range(1, len(divorces['feature_names']) + 1)])\n",
    "plot_feature_importances(classifier, labels, size=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1. Wpływ selekcji cech na efektywność klasyfikatorów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ramach zadania pierwszego każdy ze studentów powinien uruchomić pomiar skuteczności klasyfikatora K-nearest neighbors dla pewnego k, pewnej liczby cech m i określonego sposobu wyboru cech. Badane wartości to:\n",
    "\n",
    "k = \\[1, 3, 5\\]<br>\n",
    "m = \\[N, 2, 5\\]  N - pełna liczba cech (brak selekcji)<br>\n",
    "wybór cech: selekcja/losowy\n",
    "\n",
    "Każdy student powinien wykonać **dokładnie 1** pomiar. Wynik należy wpisać do odpowiedniej komórki tabeli, dostępnej pod adresem: https://docs.google.com/spreadsheets/d/1Y_qOVhLXfIi5w4YD38lMmljmqfbgDqyrm75lMnC50Oc/edit?usp=sharing\n",
    "\n",
    "(Przed uruchomieniem pomiaru, można \"zarezerwować\" odpowiednią komórkę tabeli, wpisując w nią znak '#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(dataset, m, randomly):\n",
    "    if m >= len(dataset['feature_names']):\n",
    "        return dataset['data'], dataset['feature_names']\n",
    "    else:\n",
    "        if randomly:\n",
    "            indices = np.random.choice(np.arange(len(dataset['feature_names'])), size=m, replace=False)\n",
    "        else:\n",
    "            selector = SelectKBest(f_classif, k=m)\n",
    "            selector.fit(dataset['data'], dataset['target'])\n",
    "            indices = selector.get_support(indices=True)\n",
    "        reduced_data = np.array([sample.T[indices].T for sample in dataset['data']])\n",
    "        reduced_feature_names = dataset['feature_names'][indices]\n",
    "        return reduced_data, reduced_feature_names\n",
    "\n",
    "def knn_score(dataset, k, m, random_selection=False):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    dataset_reduced = deepcopy(dataset)\n",
    "    dataset_reduced['data'], dataset_reduced['feature_names'] = select_features(dataset, m, random_selection)\n",
    "    results = cross_val_score(classifier, dataset_reduced['data'], dataset_reduced['target'], cv=5)\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wywołaj funkcję knn_score() z odpowiednimi parametrami\n",
    "knn_score(divorces, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macierz kowariancji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wariancja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wariancja** jest intuicyjnie utożsamiana ze zróżnicowaniem zbiorowości;\n",
    "jest średnią arytmetyczną kwadratów różnic poszczególnych wartości cechy od wartości oczekiwanej.\n",
    "Wariancję zmiennej losowej X oznaczamy jako:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Var(X)\\newline\n",
    "D^2(X)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obliczamy za pomocą wzoru:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "D^2(X) = E(X^2) - [E(X)]^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdzie E[X] jest wartością oczekiwaną zmiennej losowej X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kowariancja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kowariancją** nazywamy zależnośc liniową między dwowa zmiennymi losowymi X i Y.\n",
    "Kowariancję oznaczamy jako:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "cov(X,Y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wyliczamy ze wzoru:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "cov(X,Y) = E(X * Y) - E(X) * E(Y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macierz kowariancji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sposób wyliczania\n",
    "**Macierz kowariancji** jest uogólnieniem pojęcia wariancji dla przypadków wielowymiarowych. Dla wektora losowego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "(X_1,X_2,...,X_n)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma ona postać:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\Sigma =  \\begin{vmatrix}\n",
    "\\ \\sigma^2_1 & \\sigma_{12} & ... & \\sigma_{1n}  \\\\\n",
    "\\ \\sigma_{21} & \\sigma^2_2 & ... & \\sigma_{2n}  \\\\\n",
    "\\ ... & ... & ... & ... \\\\\n",
    "\\ \\sigma_{n1} & \\sigma_{n2} & ... & \\sigma^2_n\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdzie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma^2_i = D^2(X_i) - wariancja\\; zmiennej\\; X_i \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma_{ij} = cov(X_i, X_j) - kowariancja\\; miedzy\\; zmiennymi\\; losowymi\\; X_i\\; i\\; X_j\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Macierz kowariancji w Pythonie\n",
    "\n",
    "Do wyznaczania macierzy kowariancji używamy funkcji cov z biblioteki numpy. Wylicza ona macierz na podstawie podanych tablic i wag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>numpy.cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gdzie najważniejsze argumenty to:\n",
    "<ul>\n",
    "    <li> m - jedno- lub dwu- wymiarowa tablic danych, zawiarająca różne zmienne i obserwacje</li>\n",
    "    <li>bias - odpowiada za rodzaj normalizacji</li>\n",
    "    <li>fweights - jednowymiarowa tablica liczb całkowitych, wyznaczająca wagi częstotliwości, czyli ile razy dana obserwacja powinna być powtórzona</li>\n",
    "    <li>aweights - jednowymiarowa tablica, odpowiedzialna za wagi (\"ważność\") danych obserwacji.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Działanie funkcji numpy.cov\n",
    "\n",
    "Mamy daną tablicę m, gdzie kolumny są poszeczególnymi obserwacjami, niech f = fweight i a = aweight. Wyliczanie macierzy kowariancji następuje w podany sposób:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> w = f * a<br>\n",
    "=> v1 = np.sum(w)<br>\n",
    "=> v2 = np.sum(w * a)<br>\n",
    "=> m -= np.sum(m * w, axis=1, keepdims=True) / v1<br>\n",
    "=> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cancer.data[0:3,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.cov(cancer.data[0:3,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "cancer.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.cov(cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heat mapa** jest graficzną reprezentacją danych, gdzie każdy wartość elementu macierzy jest reprezentowana przez dany kolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"heat_map.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "matrix_data = np.cov(divorces['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(matrix_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Heat mapy pozwalają na łatwiejsze znalezienie obszarów o większym znaczeniu w przypadku dużej ilości danych. Są one łatwiejsze do przeanalizowania niż surowe dane liczbowe.\n",
    "\n",
    "Z takiej heat mapy możemy odczytać jak dużą kowariancją cechują się dwie zmienne losowe. Duża kowariancja między dwiema zmiennymi wskazuje, że są one wysoce „skorelowane” - zawierają informacje, które można przewidzieć lub przedstawić pojedynczą zmienną.\n",
    "\n",
    "Klasyfikowanie dużych danych bywa czasochłonne i zasobożerne. Informacje, które można wywnioskować z heat mapy pozwalają nam wyłonić z pełnego zbioru danych odpowiedni fragment, który następnie posłuży do tworzenia bardziej optymalnych klasyfikatorów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Transformacja PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformacja PCA (Principal Component Analysis) jest algorytmem analizy danych. Wykorzystuje informacje o powiązaniach pomiędzy danymi wejściowymi. Umożliwia to dokonanie selekcji i \"kompresji\" danych bez utraty istotnych informacji pierwotnego zestawu danych. PCA może być wykorzystane właśnie w problemach kompresji, analizy oraz przetwarzania złożonych zbiorów danych tak, aby wyłuszczyć z nich składaniki o największej zmienności i największym wpływie na pozostałe informacje.\n",
    "\n",
    "Transfomacja PCA jest przekształceniem liniowym Y = W * X, gdzie Y - macierz przestrzeni wyjściowe, która zachowuje najistotniejsze informacje danych wejściowych X, a W jest macierzą przekształceń PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sposób wyliczania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak w przypadku macierzy kowariancji, danymi wejściowymi jest macierz X, w której każdy wiersz to jedna obserwacja. Wielkość takiej macierzy to N x P (gdzie N - wielkość wektora danych, a  P - ilość obserwacji).\n",
    "\n",
    "Dla tak przygotowanych danych należy wyznaczyć macierz autokorelacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "R_{xx} = \\frac{1}{P} XX^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macierz Rxx posiada wymiary N × N. Informuje nas ona w jakim stopniu dane są skorelowane. Koeljnym krokiem algorytmu jest wyznaczenie wartości własnych macierzy Rxx oraz odpowiadających im wektorów własnych. Wartości te, wraz z odpowiadającymi im wektorami własnymi należy uszeregować malejąco. W ten sposób uzyskuje się macierz diagonalną:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "D =  \\begin{vmatrix}\n",
    "\\ \\lambda_1 & 0 & ... & 0  \\\\\n",
    "\\ 0 & \\lambda_2 & ... & 0  \\\\\n",
    "\\ ... & ... & ... & ... \\\\\n",
    "\\ 0 & 0 & ... & \\lambda_N\n",
    "\\end{vmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdzie:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\lambda_1 > \\lambda_2 > .. > \\lambda_N\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to wartości własne. Macierzy odpowiada macierz wektorów własnych:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "V = [v_1, v_2, ..., v_N]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Istotnym krokiem transformacji jest ocena ile najistotniejszych czynników należy uwzględnić w przekształceniu. W tym celu należy wybrać pierwszy K największych wartości własnych macierzy D, odstających znacząco od pozostałych. W ten sposób otzywamujemy macierz przekształcenia uwzględniającą K znaczących wektorów własnych:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "W = [v_1, v_2, ..., v_K]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalnie, przekształcenie PCA danych wejściowych X przedstawia się jako przekształcenie liniowe wspomniane wcześniej:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Y = WX\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacja PCA w Pythonie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do wyznaczania transformacji PCA używamy klasy PCA z sklearn.decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gdzie najważniejszy argument to n_components - ilość elemetów, które chcemy zachować w transformacji. Jeżeli nie podana, wynosi ona min(n_samples, n_features)\n",
    "\n",
    "Klasa ta posiada m.in. pola:\n",
    "\n",
    "<ul>\n",
    "    <li>components_ - główne osie w przestrzeni cech, reprezentujące kierunki maksymalnej wariancji danych</li>\n",
    "    <li>explained_variance_ - wielkość wariancji dla każdego wybranego elementu</li>\n",
    "    <li>explained_variance_ratio_ - procentowa wielkość wariancji dla każdego wybranego elementu</li>\n",
    "    <li>n_features_ - liczba cech w danych traningowych</li>\n",
    "    <li>n_samples_ - liczba próbek w danych traningowych</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja fit(self, X[, y]) dopasowuje model X.\n",
    "\n",
    "Funkcja transform(self, X) redukuje wielowymiarowość X.\n",
    "\n",
    "Funckja fit_transform(self, X[, y]) dopasowuje i redukuje wielomianowowść modelu X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit_transform(divorces['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykład działania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy przeprowadzać transformacje na pięć cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5)\n",
    "\n",
    "model = pca.fit(divorces['data'])\n",
    "model_pc = model.transform(divorces['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data = divorces['data'], columns = divorces['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data = model_pc, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4','principal component 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podstawie 40 originalnych cech powstało 5 nowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nowe cechy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.DataFrame(data = divorces['data'], columns = divorces['feature_names'])\n",
    "pca.fit_transform(divorces['data'])\n",
    "data_scaled = pd.DataFrame(preprocessing.scale(df),columns = df.columns) \n",
    "pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC1','PC2','PC3','PC4','PC5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać obie nowe cechy są pewną kombinacją originalnych cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wizualizacja ważności cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cechy originalne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier.fit(divorces['data'], divorces['target'])\n",
    "\n",
    "# treści pytań są długie, wykres etykietujemy więc samymi ich numerami\n",
    "labels = np.array(['question' + str(i) for i in range(1, len(divorces['feature_names']) + 1)])\n",
    "plot_feature_importances(classifier, labels, size=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowe cechy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'variance':pca.explained_variance_, 'PC':['PC1','PC2','PC3','PC4','PC5']})\n",
    "sns.barplot(x='PC',y=\"variance\", data=df, color=\"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2. Wpływ nowych cech na efektywność klasyfikatorów¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ramach zadania drugiego każdy ze studentów powinien uruchomić pomiar skuteczności klasyfikatora K-nearest neighbors dla pewnego k, pewnej liczby m \"nowych\" najlepszych i \"starych\" najlepszych cech. Badane wartości to:\n",
    "\n",
    "k = [1, 3, 5]\n",
    "m = [N, 2, 5] N - pełna liczba cech (brak selekcji)\n",
    "\n",
    "Gdzieś wklejmy wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(dataset, m):\n",
    "    if m >= len(dataset['feature_names']):\n",
    "        return dataset['data'], dataset['feature_names']\n",
    "    else:        \n",
    "        selector = SelectKBest(f_classif, k=m)\n",
    "        selector.fit(dataset['data'], dataset['target'])\n",
    "        indices = selector.get_support(indices=True)\n",
    "        reduced_data = np.array([sample.T[indices].T for sample in dataset['data']])\n",
    "        reduced_feature_names = dataset['feature_names'][indices]\n",
    "        return reduced_data, reduced_feature_names\n",
    "\n",
    "def knn_score(dataset, k, m):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    dataset_reduced = deepcopy(dataset)\n",
    "    dataset_reduced['data'], dataset_reduced['feature_names'] = select_features(dataset, m)\n",
    "    results = cross_val_score(classifier, dataset_reduced['data'], dataset_reduced['target'], cv=5)\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej jest przygotowanie danych i stworzenie m nowych cech zbioru `cancer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing before pca transformation\n",
    "cancer['data'] = preprocessing.scale(cancer['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "m = 5\n",
    "\n",
    "pca = PCA(n_components = m)\n",
    "cancer_pca = deepcopy(cancer)\n",
    "\n",
    "cancer_pca['data'] = model.fit_transform(cancer_pca['data'])\n",
    "#normalizing before knn\n",
    "cancer_pca['data'] = preprocessing.scale(cancer_pca['data'])\n",
    "cancer_pca['feature_names'] = asarray(['F' + str(i) for i in range(1,m+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "print(knn_score(cancer_pca, k, m))\n",
    "print(knn_score(cancer, k, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak w zadaniu poprzednim, wyniki należy wpisać pod [tym](https://docs.google.com/spreadsheets/d/1Y_qOVhLXfIi5w4YD38lMmljmqfbgDqyrm75lMnC50Oc/edit#gid=0) linkiem pod tabelą \"Wydajność klasyfikatora K-nearest neighbors dla \"nowych\" i \"starych\" cech\" tak aby wspólnie ją uzupełnić. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja zbioru danych przy pomocy PCA w dwóch wymiarach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "pca = PCA(n_components=2)\n",
    "# divorces_pca = deepcopy(divorces)\n",
    "principalComponents = pca.fit_transform(divorces['data'])\n",
    "principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "targetDf = pd.DataFrame(data = divorces['target'], columns = ['target'])\n",
    "finalDf = pd.concat([principalDf, targetDf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [1, 0]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(['Divorced', 'Not divorced'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykrycie i zaznaczenie outlierów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykrycie oultierów zostanie wykonane przez użycie Z-score. Polegać będzie to na przeskalowaniu danych i ich centralizacji oraz określeniu ich odległości od zera. Te wartości, które będą miały bezwzględny Z-score większy od pewnej wartości, np. 3, będą uznane za outliery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "z = np.abs(stats.zscore(principalDf))\n",
    "print((z > 3).any(axis = 1))\n",
    "divorces_outliers = principalDf[(z > 3).any(axis = 1)]\n",
    "divorces_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykrycie źle skwalifikowanych przykladów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train, x_test, y_train, y_test =  train_test_split(divorces['data'],divorces['target'], \n",
    "                                                     test_size=0.5,random_state=0 )\n",
    "\n",
    "x_train_scaled = preprocessing.scale(x_train)\n",
    "x_test_scaled = preprocessing.scale(x_test)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x_train_scaled)\n",
    "x_train_pca = pca.transform(x_train_scaled)\n",
    "x_test_pca = pca.transform(x_test_scaled)\n",
    "\n",
    "knc_pca = KNeighborsClassifier(n_neighbors=5, algorithm='auto').fit(x_train_pca, y_train)\n",
    "predict_y_pca = knc_pca.predict(x_test_pca)\n",
    "\n",
    "x_test_pca_df = pd.DataFrame(x_test_pca, columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "incorrects = x_test_pca_df[predict_y_pca != y_test]\n",
    "incorrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [1, 0]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['target'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.scatter(incorrects['principal component 1']\n",
    "       , incorrects['principal component 2']\n",
    "       , c = 'b'\n",
    "       , s = 50)\n",
    "ax.scatter(divorces_outliers['principal component 1']\n",
    "       , divorces_outliers['principal component 2']\n",
    "       , c = 'y'\n",
    "       , s = 50)\n",
    "ax.legend(['Divorced', 'Not divorced', 'Incorect', 'Outlier'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie końcowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W zbiorach CIFAR-10 . Do dalszych eksperymentów wybrać\n",
    "ilość przykładów nie większy niż M=2xN !!!(N- ilość cech).\n",
    "1. znaleźć NAJMNIEJ INFORMATYWNE cechy (piksele). Zobrazować je na\n",
    "rysunku, wielkością odpowiadającemu klasyfikowanym obrazkom.\n",
    "2. Dokonać klasyfikacji k-nn na pełnym zbiorze i zbiorze bez m najmniej\n",
    "informatywnych cech.\n",
    "3. Przetransformować zbiory przy pomocy PCA z N-D do N-D. Jak wyglądają\n",
    "(obrazki) wektory własne odpowiadające największym wartością własnym.\n",
    "Sprawdzić, czy poprawił się wynik klasyfikacji. Dokonać wizualizacji 2-D przy\n",
    "pomocy PCA.\n",
    "4. Usunąć m najmniej informatywnych cech PCA. Jak wygląda wynik klasyfikacji.\n",
    "5. Wybrac m NAJLEPSZYCH cech PCA. Jak wygląda teraz wynik klasyfikacji.\n",
    "6. Wartość m w przypadku wyboru najgorszych cech ma być duże (dla N=784\n",
    "jakieś m=500), w przypadku wyboru najlepszych małe (m=10-20)\n",
    "7. Dokonać klasyfikacji z PCA i bez PCA (na pełnym zbiorze cech i zadanym małym\n",
    "M), ale zwiększając ilość przykładów przy pomocy augmentacji (imgaug).\n",
    "8. Wnioski (co lepsze augmentacja czy inżynieria cech?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
