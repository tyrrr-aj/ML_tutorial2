{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podstawy uczenia maszynowego - tutorial 2: Feature selection and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie zbiorów danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zbiór danych medycznych pacjentów pozwalający na diagnostykę raka płuc\n",
    "\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "#cancer_train_x, cancer_train_y, cancer_test_x, cancer_test_y = cancer.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zbiór danych pozwalający przewidywać, czy małżeństwo się rozpadnie na podstawie odpowiedzi (w skali 1-5) na 56 pytań\n",
    "\n",
    "divorces = pd.read_csv('./datasets/divorce.csv', sep=';')\n",
    "\n",
    "# treści pytań są przechowywane w osobnym pliku\n",
    "with open('./datasets/questions.csv') as file:\n",
    "    questions = np.array(file.read().split('\\n'))\n",
    "\n",
    "divorces = {\n",
    "    'data': divorces.drop(labels=['Class'], axis=1),\n",
    "    'target': list(divorces['Class']),\n",
    "    'feature_names': questions\n",
    "}\n",
    "\n",
    "#divorces_train_x, divorces_train_y, divorces_test_x, divorces_test_y = divorces.train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selekcja cech to ograniczenie liczby atrybutów danych, na których pracuje model, przez odrzucenie najmniej użytecznych z nich. Taka rezygnacja z części danych pozwala osiągnąć konkretne korzyści:\n",
    "* redukcja wymiarów - odrzucenie części cech oznacza zmniejszenie wymiarowości problemu, co ułatwia uniknięcie przetrenowania modelu\n",
    "* uproszczenie modelu - model pracujący na mniejszej liczbie cech jest bardziej zrozumiały i łatwiej identyfikować w nim problemy\n",
    "* lepsze wyniki - cechy niezwiązane z badaną właściwością mogą w sposób losowy zaburzać otrzymywane wyniki\n",
    "* poprawa wydajności - mało istotne cechy są przetwarzane niepotrzebnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metody Selekcji cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Odrzucenie cech niecharakterystycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cechy, które wykazują niewielką zmienność pomiędzy próbkami, prawdopodobnie nie niosą szczególnie użytecznej informacji - zazwyczaj nie można na ich podstawie dokonać klasyfikacji (choć warto zachować ostrożność przy zbiorach danych o silnej dysproporcji pomiędzy licznościami klas oraz cechach mogących przyjmować wartości z niewielkiego zakresu). Cechy takie można rozpoznać po niewielkiej wariancji - stąd najprostsze podejście do selekcji cech może polegać na ich przefiltrowaniu i odrzuceniu tych o zbyt niskiej wariancji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance should equal, because ...\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "cancer.data = selector.fit_transform(cancer.data)\n",
    "\n",
    "\n",
    "# lista atrybutów wymaga zaktualizowania, selector.get_support() zwraca indeksy wybranych cech\n",
    "cancer['feature_names'] = cancer['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold()\n",
    "divorces['data'] = selector.fit_transform(divorces['data'])\n",
    "\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Odrzucenie cech niezwiązanych z badaną właściwością"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy spróbować przewidzieć, na ile każda z cech jest związana z badaną właściwością, i odrzucić te, dla których związek jest luźny. Możliwe są dwa zasadnicze podejścia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Obliczenie korelacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najprostszym sposobem określenia, czy cecha jest związana z inną (w szczególności - przynależnością do danej klasy) jest obliczenie korelacji między nimi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correlation(X, Y, feature_names, n_features):\n",
    "    scores = [abs(np.corrcoef(feature, Y))[0, 1] for feature in X.T]\n",
    "    selected_indices = np.argsort(scores)[:n_features]\n",
    "    data = X.T[selected_indices].T\n",
    "    selected_feature_names = feature_names[selected_indices]\n",
    "    return data, selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.data, cancer.feature_names = filter_correlation(cancer.data, cancer.target, cancer.feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_correation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-0d95ac069565>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdivorces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivorces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfilter_correation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivorces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivorces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivorces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'feature_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'filter_correation' is not defined"
     ]
    }
   ],
   "source": [
    "divorces['data'], divorces['feature_names'] = \\\n",
    "    filter_correation(divorces['data'], divorces['target'], divorces['feature_names'], 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2. Inne statystyki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatywnie, możemy wykorzystać inną statystykę, której wartość rośnie wraz ze wzrostem różnicy wartości w dwóch grupach - np. Chi-kwadrat. \n",
    "\n",
    "W tym celu zakładamy, że badana cecha (nazwijmy ją A) nie ma związku z przynależnością próbki do określonej klasy (oznaczmy ją przez C). Opierając się na tym założeniu, obliczamy, ile próbek z każdą możliwą wartością cechy A powinno należeć do klasy C (jako liczba próbek o danej wartości cechy A * liczba próbek w ekperymencie należących do klasy C / liczba wszystkich próbek),\n",
    "i obliczamy wartość wybranej statystyki na podstawie różnic pomiędzy otrzymanymi wartościami a rzeczywistymi danymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=20)\n",
    "\n",
    "cancer.data = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=30)\n",
    "\n",
    "divorces['data'] = selector.fit_transform(divorces['data'], divorces['target'])\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Próbne wytrenowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Odfiltrowanie cech nieznaczących"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast \"ręcznie\" znajdować i usuwać mało istotne cechy, możemy spróbować wytrenować na naszych danych model, który ucząc się \"przy okazji\" zapisuje istotność cech, i usunąć te, które nie mają dużego znaczenia w podejmowaniu decyzji przez model. Aby zastosować takie podejści, konieczny jest wybór modelu, który udostępnia istotność cech - w przypadku Scikit-learn są to modele posiadające pole coef_ lub feature_importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "classifier = classifier.fit(cancer.data, cancer.target)\n",
    "selector = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "cancer.data = selector.transform(cancer.data)\n",
    "cancer.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "classifier = classifier.fit(divorces['data'], divorces['target'])\n",
    "selector = SelectFromModel(classifier, prefit=True)\n",
    "\n",
    "divorces['data'] = selector.transform(divorces['data'])\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Wybór zadanej liczby najlepszych cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli chcemy ograniczyć liczbę cech do konkretnej wartości, powyższe podejście można zmodyfikować: zamiast wybierać cechy powyżej określonej granicy, lepszym podejściem może być odrzucenie najsłabszej z cech (lub kilku najgorszych) i rekurencyjne powtarzanie procesu, aż do osiągnięcia zadanej ich liczby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ExtraTreesClassifier(n_estimators=50)\n",
    "selector = RFE(estimator, 20, step=3)\n",
    "\n",
    "cancer.data = selector.fit_transform(cancer.data, cancer.target)\n",
    "cancer.feature_names = cancer.feature_names[selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ExtraTreesClassifier(n_estimators=50)\n",
    "selector = RFE(estimator, 30, step=3)\n",
    "\n",
    "divorces['data'] = selector.fit_transform(divorces['data'], divorces['target'])\n",
    "divorces['feature_names'] = divorces['feature_names'][selector.get_support(indices=True)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
